{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import math\n",
    "import networkx\n",
    "import PIconnect as PI\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIconnect as PI\n",
    "def pi_connect_and_pulldata(df_PITag, startDate, endDate, interval):\n",
    "    \n",
    "    \"\"\"\n",
    "    example       : pi_connect_and_pulldata(df,'7-Jun-16 06:45:00','7-Jun-16 07:00:00','15s')\n",
    "    df_PITag      : pandas dataframe containing PI Tag need to pull\n",
    "    startDate     : start date\n",
    "    endDate       : end date\n",
    "    interval      : how frequency\n",
    "    \"\"\"\n",
    "    \n",
    "    df_return = pd.DataFrame()\n",
    "    \n",
    "   \n",
    "    for index, row in df_PITag.iterrows():\n",
    "        print(row['Name'])\n",
    "        #with PI.PIServer(server='pigilcollective') as server:\n",
    "        with PI.PIServer() as server:\n",
    "            try:\n",
    "                points = server.search(row['Name'])[0]\n",
    "                data = points.interpolated_values(startDate,endDate,interval) \n",
    "                #data = points.interpolated_values('7-Jun-16 06:45:00','7-Jun-16 07:00:00','15s') \n",
    "                if(isinstance(data.values[0], np.floating) & isinstance(data.values[0], float)):\n",
    "                    data.name = row['Combined_tags']\n",
    "                    df_return = df_return.append(data)\n",
    "            except Exception:\n",
    "                print('Error somewhere in pi_connect_and_pulldata')\n",
    "                pass\n",
    "    \n",
    "    return df_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import json\n",
    "\n",
    "def export_tree_json(array_linkage, labels, current_time):\n",
    "    \n",
    "    id2name = dict(enumerate(labels))\n",
    "    T = shc.to_tree(array_linkage , rd=False )\n",
    "\n",
    "    # Create a nested dictionary from the ClusterNode's returned by SciPy\n",
    "    def add_node(node, parent ):\n",
    "        # First create the new node and append it to its parent's children\n",
    "        newNode = dict( node_id=node.id, children=[] )\n",
    "        parent[\"children\"].append( newNode )\n",
    "\n",
    "        # Recursively add the current node's children\n",
    "        if node.left: add_node( node.left, newNode )\n",
    "        if node.right: add_node( node.right, newNode )\n",
    "\n",
    "    # Initialize nested dictionary for d3, then recursively iterate through tree\n",
    "    #d3Dendro = dict(children=[], name=\"Root1\")\n",
    "    #add_node( T, d3Dendro )\n",
    "\n",
    "    # Label each node with the names of each leaf in its subtree\n",
    "    def label_tree( n ):\n",
    "        # If the node is a leaf, then we have its name\n",
    "        if len(n[\"children\"]) == 0:\n",
    "            leafNames = [ id2name[n[\"node_id\"]] ]\n",
    "\n",
    "        # If not, flatten all the leaves in the node's subtree\n",
    "        else:\n",
    "            leafNames = reduce(lambda ls, c: ls + label_tree(c), n[\"children\"], [])\n",
    "\n",
    "        # Delete the node id since we don't need it anymore and\n",
    "        # it makes for cleaner JSON\n",
    "        del n[\"node_id\"]\n",
    "\n",
    "        # Labeling convention: \"-\"-separated leaf names\n",
    "        #n[\"name\"] = name = \"-\".join(sorted(map(str, leafNames)))\n",
    "        n[\"name\"] = name = sorted(leafNames)[-1] \n",
    "        #n[\"name\"] = name = sorted(leafNames)[0] \n",
    "        return leafNames\n",
    "\n",
    "    \n",
    "    # Initialize nested dictionary for d3, then recursively iterate through tree\n",
    "    d3Dendro = dict(children=[], name=\"Root1\")\n",
    "    add_node( T, d3Dendro )\n",
    "\n",
    "    # Create Tree Dendogram\n",
    "    label_tree( d3Dendro[\"children\"][0] )\n",
    "\n",
    "    # Output to JSON\n",
    "    json.dump(d3Dendro, open(current_time+\".json\", \"w\"), sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "def grangers_causation_topN_list(first_out_tag, data\n",
    "                                  , test_score_method ='ssr_chi2test'\n",
    "                                  , decimal_round_digit=9\n",
    "                                  , maxlag=9\n",
    "                                  , topN=20): \n",
    "    \n",
    "    #first_out_tag = 'LIT2060_PV'\n",
    "    #overallTags = ['LIT2060_PV','LIT2061_PV','LIT2062_PV']\n",
    "    input_df = data\n",
    "    overall_tags = input_df.columns\n",
    "\n",
    "    temp_df = pd.DataFrame(np.zeros((1, len(overall_tags))), columns=overall_tags, index=[first_out_tag])\n",
    "    for y in temp_df.columns:\n",
    "        #print(y)\n",
    "        try:\n",
    "            granger_result = grangercausalitytests(input_df[[first_out_tag, y]], maxlag=maxlag, verbose=False)\n",
    "            p_values = [round(granger_result[i+1][0][test_score_method][1],decimal_round_digit) for i in range(maxlag)]\n",
    "            min_p_value = np.min(p_values)\n",
    "            temp_df.loc[first_out_tag, y] = min_p_value\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "    return temp_df.transpose()[first_out_tag].sort_values().head(topN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_PITag=pd.read_csv(r'C:\\Users\\szjt\\Auto_AD\\PI_Tag_Reduced.csv', encoding= 'unicode_escape')\n",
    "def get_tag_data(area, level_YES, main_category, sub_category):\n",
    "    df_PITag = pd.read_csv(r'C:\\Users\\szjt\\Auto_AD\\Final_'+area+'_Tags.csv')\n",
    "    df_PITag[\"Name\"] = df_PITag[\"Name\"].str.replace('PIGILCollective','')\n",
    "    df_PITag[\"Name\"] = df_PITag[\"Name\"].str.replace('\\\\\\\\','')\n",
    "    \n",
    "    if(level_YES):\n",
    "        df_PITag = df_PITag[(df_PITag['Level'] == 'Y')]\n",
    "    else:\n",
    "        df_PITag = df_PITag[(df_PITag['Main category'].isin(main_category)) & (df_PITag['Sub category'].isin(sub_category))]\n",
    "    \n",
    "    df_PITag.rename(columns={'Sensor': 'Name'}, inplace=True)\n",
    "   \n",
    "    print(df_PITag.head())\n",
    "    return df_PITag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_inverse_data(input_df, df_PITag_Value):\n",
    "    \n",
    "    df_PITag_inverse = df_PITag_Value[(df_PITag_Value[\"Valve\"]=='Yes')]\n",
    "    list_tags = df_PITag_inverse['Combined_tags'].tolist()\n",
    "    update_df = pd.DataFrame()\n",
    "    for index, row in input_df.iterrows():\n",
    "        if any(index in s for s in list_tags):\n",
    "            row_new = 100 - row\n",
    "            #row_new = 1/row.add(1)\n",
    "            row_new.name = row.name +'_Converted'\n",
    "            update_df = update_df.append(row_new,ignore_index=False)\n",
    "\n",
    "    return update_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as shc\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "#inputData = pd.read_csv(r\"C:\\Users\\tsvd\\PIdataJuly2020.csv\")\n",
    "\n",
    "def autoAD_clustering(start_date, end_date, frequency_method, selected_CPP, level_chkbox, tags_main_picker, tags_sub_picker, granger_picker, granger_txt):\n",
    "    \n",
    "    print(\"Get specific Tags from UI\")\n",
    "    df_PITag = get_tag_data(selected_CPP, level_chkbox, tags_main_picker, tags_sub_picker)\n",
    "    print(df_PITag.head())\n",
    "    print(\"Get PI Data from Date Range, Frequency and Tags Selections\")\n",
    "    original_df = pi_connect_and_pulldata(df_PITag, start_date, end_date, frequency_method)\n",
    "    original_df.to_csv(selected_CPP+\"_\"+datetime.now().strftime(\"%Y_%m_%d_%H_%M\"+\".csv\"))\n",
    "    #original_df = pd.read_csv(r\"C:\\Users\\tsvd\\Jo_auto_ad\\SACPP_PI_DATA_edited.csv\")\n",
    "    print(original_df.head())\n",
    "    print(\"Convert data to (1/(1+x))\")\n",
    "    df_convert_inverse_data = convert_inverse_data(original_df, df_PITag)\n",
    "    df_convert_inverse_data.to_csv(selected_CPP+\"_converted_inversed\"+datetime.now().strftime(\"%Y_%m_%d_%H_%M\"+\".csv\"))\n",
    "    original_df = pd.concat([df_convert_inverse_data, original_df])\n",
    "    print(original_df.head())\n",
    "    original_df.to_csv(selected_CPP+\"_converted_\"+datetime.now().strftime(\"%Y_%m_%d_%H_%M\"+\".csv\"))\n",
    "    print(\"Data Preprocessing\")\n",
    "    print(original_df.info())\n",
    "    original_df.index = original_df.index.str.replace(selected_CPP+'_','')\n",
    "    original_df = original_df.transpose()\n",
    "    #original_df.head()\n",
    "    print('Transposing data frame')\n",
    "    print(original_df.info())\n",
    "    scaler = MinMaxScaler()\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(original_df), columns=original_df.columns)\n",
    "    df_scaled = df_scaled.loc[:, (df_scaled != 0).any(axis=0)]\n",
    "    #df_scaled.head()\n",
    "    print('Scaling Data')\n",
    "    print(df_scaled.info())\n",
    "    df_scaled = df_scaled.dropna(axis = \"columns\")\n",
    "    df_scaled.to_csv(selected_CPP+\"_scaled_\"+datetime.now().strftime(\"%Y_%m_%d_%H_%M\"+\".csv\"))\n",
    "    print(\"Method Activated!!!\")\n",
    "    print(granger_picker)\n",
    "    if(granger_picker == \"Granger Causality Test\"):\n",
    "    #if any(\"Granger Casuality Test\" in item for item in granger_picker):\n",
    "        \n",
    "        result_msg = grangers_causation_topN_list(granger_txt, df_scaled, decimal_round_digit=9, topN=20, maxlag=6).to_string()\n",
    "        print(\"Granger Activated!!!\")\n",
    "        print(result_msg)\n",
    "        #result_msg = \"Granger Completed!!!\"\n",
    "    else :\n",
    "        \n",
    "        #fig = plt.figure(figsize=(15, 15))\n",
    "        #plt.title(\"Customer Dendograms\")\n",
    "\n",
    "        Z = shc.linkage(df_scaled, method = 'ward', metric = 'euclidean')\n",
    "        #print(Z)\n",
    "        # Plot with Custom leaves\n",
    "        ax = shc.dendrogram(Z, orientation='left', labels=df_scaled.columns)\n",
    "\n",
    "        id2name = dict(enumerate(df_scaled.columns))\n",
    "        print(\"Export .json file\")\n",
    "        export_tree_json(Z, df_scaled.columns, selected_CPP+\"_\"+datetime.now().strftime(\"%Y_%m_%d_%H_%M\"))\n",
    "              \n",
    "        result_msg = \"Clustering Completed!!!\"\n",
    "    \n",
    "    print(\"!!!AUTO-AD DONE!!!\")\n",
    "    \n",
    "    return result_msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gradio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-c2b6911c291c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgradio\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m sub_category = ['Separators',\n\u001b[0;32m      4\u001b[0m \u001b[1;34m'Air System'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;34m'Condensate Stabilization'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gradio'"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "sub_category = ['Separators',\n",
    "'Air System',\n",
    "'Condensate Stabilization',\n",
    "'Air coolers',\n",
    "'Heat Exchangers',\n",
    "'Filter Coalescers',\n",
    "'Filters',\n",
    "'Compressors',\n",
    "'Generators',\n",
    "'Glycol',\n",
    "'Heaters',\n",
    "'Hydrocyclones',\n",
    "'IGFs',\n",
    "'Launchers',\n",
    "'Pumps',\n",
    "'Receivers',\n",
    "'Sales Gas Meters',\n",
    "'Storage Tanks',\n",
    "'WHRU',\n",
    "'PWIP'\n",
    "]\n",
    "#Shown time in CSV file will be Thailand time -7h\n",
    "start_date = gr.inputs.Textbox(lines=1, default=\"07/05/20 00:00\", label=\"Start Date (Month/Day/Year Thailand time)\")\n",
    "end_date = gr.inputs.Textbox(lines=1, default=\"07/05/20 21:00\", label=\"End Date (Month/Day/Year Thailand time)\")\n",
    "frequency_method = gr.inputs.Dropdown(['15s', '1m', '15m','1h','1d'], label=\"Sampling frequency\")\n",
    "selected_CPP = gr.inputs.Radio(['SACPP', 'PACPP', 'Benchamas', 'NPCPP'], label=\"Select CPP\")\n",
    "level_chkbox = gr.inputs.Checkbox(label=\"Level?\")\n",
    "tags_main_picker = gr.inputs.CheckboxGroup(['Gas_Processing','Liquid_Processing','Utility'], label=\"Main category\")\n",
    "tags_sub_picker = gr.inputs.CheckboxGroup(sub_category, label=\"Sub category\")\n",
    "granger_picker = gr.inputs.Radio(['Hierarchical Clustering', 'Granger Causality Test'],label='Method')\n",
    "granger_txt = gr.inputs.Textbox(lines=1, label='First Out Tag (Required for Granger Causality Test Only) Example: CEN_LIC2720_PV LIQUID LEVEL')\n",
    "\n",
    "gr.Interface(\n",
    "    autoAD_clustering, \n",
    "    [\n",
    "        start_date, \n",
    "        end_date, \n",
    "        frequency_method,\n",
    "        selected_CPP,\n",
    "        level_chkbox,\n",
    "        tags_main_picker,\n",
    "        tags_sub_picker,\n",
    "        granger_picker,\n",
    "        granger_txt\n",
    "    ], \n",
    "    #gr.outputs.Image(plot=True, label=\"clustering\"), title=\"AUTO-AD\",\n",
    "    gr.outputs.Textbox(label=\"Result\"),\n",
    "    allow_flagging = False).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\szjt\\OneDrive - Chevron\\Desktop\\Working Files\\Satun Troubleshooting\\Hot Oil\\HotOilForClustering.csv\")\n",
    "df = df.drop(['Unnamed: 0','Date'], axis = 1)\n",
    "df = df.dropna(axis = \"columns\")\n",
    "print(df.shape)\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "df_scaled = df_scaled.loc[:, (df_scaled != 0).any(axis=0)]\n",
    "print(df_scaled)\n",
    "Z = shc.linkage(df_scaled.T, method = 'ward', metric = 'euclidean')\n",
    "ax = shc.dendrogram(Z, orientation='left', labels=df_scaled.columns)\n",
    "id2name = dict(enumerate(df_scaled.columns))\n",
    "print(\"Export .json file\")\n",
    "export_tree_json(Z, df_scaled.columns, \"SACPP\"+\"_Hotoil_\"+datetime.now().strftime(\"%Y_%m_%d_%H_%M\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
